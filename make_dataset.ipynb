{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the dataset of research papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Entrez](http://biopython.org/DIST/docs/api/Bio.Entrez-module.html) module, a part of the [Biopython](http://biopython.org/) library, will be used to interface with [PubMed](http://www.ncbi.nlm.nih.gov/pubmed).<br>\n",
    "You can download Biopython from [here](http://biopython.org/wiki/Download).\n",
    "\n",
    "In this notebook we will be covering several of the steps taken in the [Biopython Tutorial](http://biopython.org/DIST/docs/tutorial/Tutorial.html), specifically in [Chapter 9  Accessing NCBI’s Entrez databases](http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in /home/rss220/.local/lib/python3.8/site-packages (1.81)\n",
      "Requirement already satisfied: numpy in /home/rss220/.local/lib/python3.8/site-packages (from biopython) (1.24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "\n",
    "# NCBI requires you to set your email address to make use of NCBI's E-utilities\n",
    "Entrez.email = \"r.m.siebes@vu.nl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets will be saved as serialized Python objects, compressed with bzip2.\n",
    "Saving/loading them will therefore require the [pickle](http://docs.python.org/3/library/pickle.html) and [bz2](http://docs.python.org/3/library/bz2.html) modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle, bz2, os, zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EInfo: Obtaining information about the Entrez databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# accessing extended information about the PubMed database\n",
    "pubmed = Entrez.read( Entrez.einfo(db=\"pubmed\"), validate=False )[u'DbInfo']\n",
    "\n",
    "# list of possible search fields for use with ESearch:\n",
    "search_fields = { f['Name']:f['Description'] for f in pubmed[\"FieldList\"] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In search_fields, we find 'TIAB' ('Free text associated with Abstract/Title') as a possible search field to use in searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ALL': 'All terms from all searchable fields',\n",
       " 'UID': 'Unique number assigned to publication',\n",
       " 'FILT': 'Limits the records',\n",
       " 'TITL': 'Words in title of publication',\n",
       " 'MESH': 'Medical Subject Headings assigned to publication',\n",
       " 'MAJR': 'MeSH terms of major importance to publication',\n",
       " 'JOUR': 'Journal abbreviation of publication',\n",
       " 'AFFL': \"Author's institutional affiliation and address\",\n",
       " 'ECNO': 'EC number for enzyme or CAS registry number',\n",
       " 'SUBS': 'CAS chemical name or MEDLINE Substance Name',\n",
       " 'PDAT': 'Date of publication',\n",
       " 'EDAT': 'Date publication first accessible through Entrez',\n",
       " 'VOL': 'Volume number of publication',\n",
       " 'PAGE': 'Page number(s) of publication',\n",
       " 'PTYP': 'Type of publication (e.g., review)',\n",
       " 'LANG': 'Language of publication',\n",
       " 'ISS': 'Issue number of publication',\n",
       " 'SUBH': 'Additional specificity for MeSH term',\n",
       " 'SI': 'Cross-reference from publication to other databases',\n",
       " 'MHDA': 'Date publication was indexed with MeSH terms',\n",
       " 'TIAB': 'Free text associated with Abstract/Title',\n",
       " 'OTRM': 'Other terms associated with publication',\n",
       " 'COLN': 'Corporate Author of publication',\n",
       " 'CNTY': 'Country of publication',\n",
       " 'PAPX': 'MeSH pharmacological action pre-explosions',\n",
       " 'GRNT': 'NIH Grant Numbers',\n",
       " 'MDAT': 'Date of last modification',\n",
       " 'CDAT': 'Date of completion',\n",
       " 'PID': 'Publisher ID',\n",
       " 'FAUT': 'First Author of publication',\n",
       " 'FULL': 'Full Author Name(s) of publication',\n",
       " 'FINV': 'Full name of investigator',\n",
       " 'TT': 'Words in transliterated title of publication',\n",
       " 'LAUT': 'Last Author of publication',\n",
       " 'PPDT': 'Date of print publication',\n",
       " 'EPDT': 'Date of Electronic publication',\n",
       " 'LID': 'ELocation ID',\n",
       " 'CRDT': 'Date publication first accessible through Entrez',\n",
       " 'BOOK': 'ID of the book that contains the document',\n",
       " 'ED': \"Section's Editor\",\n",
       " 'ISBN': 'ISBN',\n",
       " 'PUBN': \"Publisher's name\",\n",
       " 'AUCL': 'Author Cluster ID',\n",
       " 'EID': 'Extended PMID',\n",
       " 'DSO': 'Additional text from the summary',\n",
       " 'AUID': 'Author Identifier',\n",
       " 'PS': 'Personal Name as Subject',\n",
       " 'COIS': 'Conflict of Interest Statements',\n",
       " 'WORD': 'Free text associated with publication',\n",
       " 'P1DAT': 'Date publication first accessible through Solr'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESearch: Searching the Entrez databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a look at the kind of data we get when searching the database, we'll perform a search for papers authored by Haasdijk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Count': '41', 'RetMax': '20', 'RetStart': '0', 'IdList': ['39325889', '38631313', '33501027', '33501026', '33500899', '29311830', '28513205', '28513201', '28323435', '28140628', '26933487', '24977986', '24901702', '24852945', '24708899', '24252306', '23580075', '23144668', '22174697', '22154920'], 'TranslationSet': [{'From': 'Haasdijk E[AUTH]', 'To': 'haasdijk e[Author]'}], 'QueryTranslation': 'haasdijk e[Author]'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_authors = ['Haasdijk E']\n",
    "example_search = Entrez.read( Entrez.esearch( db=\"pubmed\", term=' AND '.join([a+'[AUTH]' for a in example_authors]) ) )\n",
    "example_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the result being produced is not in Python's native string format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bio.Entrez.Parser.StringElement"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type( example_search['IdList'][0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part of the query's result we are most interested in is accessible through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39325889, 38631313, 33501027, 33501026, 33500899, 29311830, 28513205, 28513201, 28323435, 28140628, 26933487, 24977986, 24901702, 24852945, 24708899, 24252306, 23580075, 23144668, 22174697, 22154920]\n"
     ]
    }
   ],
   "source": [
    "example_ids = [ int(id) for id in example_search['IdList'] ]\n",
    "print(example_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed IDs dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now assemble a dataset comprised of research articles containing the given keyword, in either their titles or abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Due to the new limit (only 10K max results) the Entrez library is not suitable to fetch all ids for a query\n",
    "#  - Therefore this has to be done via the more complicated way using the Entrez Direct e-utility:\n",
    "#     https://www.ncbi.nlm.nih.gov/sites/books/NBK179288/ \n",
    "#  - also an API key has to be registered at NCBI\n",
    "#  - the command used for fetching the ids: \n",
    "#               esearch -db pubmed -query \"recognition\" | efetch -format uid >results.txt\n",
    "#  - thus, instead of using the output from the script below (recognition_ids.pkl.bz2) we use\n",
    "search_term = 'recognition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ids_zip = 'data/' + search_term + '_Ids.zip'\n",
    "Ids_file = search_term + '_Ids.txt'\n",
    "\n",
    "if os.path.exists( Ids_zip ):\n",
    "    with zipfile.ZipFile(Ids_zip, 'r') as myzip:\n",
    "        \n",
    "        with myzip.open(Ids_file) as myfile:\n",
    "           Ids_str = myfile.read().decode('UTF-8').splitlines(keepends=True)\n",
    "\n",
    "        # convert Ids to integers (and ensure that the conversion is reversible)\n",
    "        Ids = [ int(id) for id in Ids_str ]\n",
    "else:\n",
    "     raise Exception('File not found' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481216 documents contain the search term \"recognition\".\n"
     ]
    }
   ],
   "source": [
    "ids_to_remove = [\n",
    "    39295396\n",
    "]\n",
    "for id_to_remove in ids_to_remove:\n",
    "    try:\n",
    "        Ids.remove(id_to_remove)\n",
    "    except ValueError:\n",
    "        print(f\"ID {id_to_remove} not found in the list.\")\n",
    "\n",
    "    \n",
    "    # Save list of Ids\n",
    "    pickle.dump( Ids, bz2.BZ2File( Ids_file, 'wb' ) )\n",
    "    \n",
    "total = len( Ids )\n",
    "print('%d documents contain the search term \"%s\".' % ( total, search_term ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at what we just retrieved, here are the last 5 elements of the `Ids` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53, 349, 480, 1346, 1635]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "Ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESummary: Retrieving summaries from primary IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a look at the kind of metadata we get from a call to `Entrez.esummary()`, we now fetch the summary of one of Haasdijk's papers (using one of the PubMed IDs we obtained in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item\n",
      "\t []\n",
      "Id\n",
      "\t 1310895\n",
      "PubDate\n",
      "\t 1992 Feb 7\n",
      "EPubDate\n",
      "\t \n",
      "Source\n",
      "\t Cell\n",
      "AuthorList\n",
      "\t ['Bölker M', 'Urban M', 'Kahmann R']\n",
      "LastAuthor\n",
      "\t Kahmann R\n",
      "Title\n",
      "\t The a mating type locus of U. maydis specifies cell signaling components.\n",
      "Volume\n",
      "\t 68\n",
      "Issue\n",
      "\t 3\n",
      "Pages\n",
      "\t 441-50\n",
      "LangList\n",
      "\t ['English']\n",
      "NlmUniqueID\n",
      "\t 0413066\n",
      "ISSN\n",
      "\t 0092-8674\n",
      "ESSN\n",
      "\t \n",
      "PubTypeList\n",
      "\t ['Journal Article']\n",
      "RecordStatus\n",
      "\t PubMed - indexed for MEDLINE\n",
      "PubStatus\n",
      "\t ppublish\n",
      "ArticleIds\n",
      "\t {'pubmed': ['1310895'], 'medline': [], 'doi': '10.1016/0092-8674(92)90182-c', 'pii': '0092-8674(92)90182-C'}\n",
      "DOI\n",
      "\t 10.1016/0092-8674(92)90182-c\n",
      "History\n",
      "\t {'pubmed': ['1992/02/07 00:00'], 'medline': ['1992/02/07 00:01'], 'entrez': '1992/02/07 00:00'}\n",
      "References\n",
      "\t []\n",
      "HasAbstract\n",
      "\t IntegerElement(1, attributes={})\n",
      "PmcRefCount\n",
      "\t IntegerElement(0, attributes={})\n",
      "FullJournalName\n",
      "\t Cell\n",
      "ELocationID\n",
      "\t \n",
      "SO\n",
      "\t 1992 Feb 7;68(3):441-50\n"
     ]
    }
   ],
   "source": [
    "example_paper = Entrez.read( Entrez.esummary(db=\"pubmed\", id='1310895') )[0]\n",
    "\n",
    "def print_dict( p ):\n",
    "    for k,v in p.items():\n",
    "        print(k)\n",
    "        print('\\t', v)\n",
    "\n",
    "print_dict(example_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll keep just some basic information for each paper: title, list of authors, publication year, and [DOI](https://en.wikipedia.org/wiki/Digital_object_identifier).\n",
    "\n",
    "In case you are not familiar with the DOI system, know that the paper above can be accessed through the link  `https://www.doi.org/` followed by the paper's DOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The a mating type locus of U. maydis specifies cell signaling components.',\n",
       " ['Bölker M', 'Urban M', 'Kahmann R'],\n",
       " 1992,\n",
       " '10.1016/0092-8674(92)90182-c')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( example_paper['Title'], example_paper['AuthorList'], int(example_paper['PubDate'][:4]), example_paper['DOI'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summaries dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to assemble a dataset containing the summaries of all the paper `Ids` we previously fetched.\n",
    "\n",
    "To reduce the memory footprint, and to ensure the saved datasets won't depend on Biopython being installed to be properly loaded, values returned by `Entrez.read()` will be converted to their corresponding native Python types. We start by defining a function for helping with the conversion of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Summaries_file = 'data/' + search_term + '_Summaries.pkl.bz2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Summaries of results: \n",
      "\n",
      "0length= 0\n",
      "...................\n",
      "10000length= 10000\n",
      "...................\n",
      "20000length= 20000\n",
      "...................\n",
      "30000length= 30000\n",
      "...................\n",
      "40000length= 40000\n",
      "...................\n",
      "50000length= 50000\n",
      "...................\n",
      "60000length= 60000\n",
      "...................\n",
      "70000length= 70000\n",
      "...................\n",
      "80000length= 80000\n",
      "...................\n",
      "90000length= 90000\n",
      "...................\n",
      "100000length= 100000\n",
      "...................\n",
      "110000length= 110000\n",
      "...................\n",
      "120000length= 120000\n",
      "...................\n",
      "130000length= 130000\n",
      "...................\n",
      "140000length= 140000\n",
      "...................\n",
      "150000length= 150000\n",
      "...................\n",
      "160000length= 160000\n",
      "...................\n",
      "170000length= 170000\n",
      "...................\n",
      "180000length= 180000\n",
      "...................\n",
      "190000length= 190000\n",
      "...................\n",
      "200000length= 200000\n",
      "...................\n",
      "210000length= 210000\n",
      "...................\n",
      "220000length= 220000\n",
      "...................\n",
      "230000length= 230000\n",
      "...................\n",
      "240000length= 240000\n",
      "...................\n",
      "250000length= 250000\n",
      "...................\n",
      "260000length= 260000\n",
      "...................\n",
      "270000length= 270000\n",
      "...................\n",
      "280000length= 280000\n",
      "...................\n",
      "290000length= 290000\n",
      "...................\n",
      "300000length= 300000\n",
      "...................\n",
      "310000length= 310000\n",
      "...................\n",
      "320000length= 320000\n",
      "...................\n",
      "330000length= 330000\n",
      "...................\n",
      "340000length= 340000\n",
      "...................\n",
      "350000length= 350000\n",
      "...................\n",
      "360000length= 360000\n",
      "...................\n",
      "370000length= 370000\n",
      "...................\n",
      "380000length= 380000\n",
      "...................\n",
      "390000length= 390000\n",
      "...................\n",
      "400000length= 400000\n",
      "...................\n",
      "410000length= 410000\n",
      "...................\n",
      "420000length= 420000\n",
      "...................\n",
      "430000length= 430000\n",
      "...................\n",
      "440000length= 440000\n",
      "...................\n",
      "450000length= 450000\n",
      "...................\n",
      "460000length= 460000\n",
      "...................\n",
      "470000length= 470000\n",
      "...................\n",
      "480000length= 480000\n",
      ".."
     ]
    }
   ],
   "source": [
    "if os.path.exists( Summaries_file ):\n",
    "    Summaries = pickle.load( bz2.BZ2File( Summaries_file, 'rb' ) )\n",
    "    \n",
    "else:\n",
    "    # `Summaries` will be incrementally assembled, by performing multiple queries,\n",
    "    # each returning at most `retrieve_per_query` entries.\n",
    "    Summaries = []\n",
    "    retrieve_per_query = 500\n",
    "    \n",
    "    print('Fetching Summaries of results: ')\n",
    "    for start in range( 0, len(Ids), retrieve_per_query ):\n",
    "        if (start % 10000 == 0):\n",
    "            print('')\n",
    "            print(start, end='')\n",
    "            SummariesDict = dict( Summaries )\n",
    "            print('length=', len(SummariesDict))\n",
    "        else:\n",
    "            print('.', end='')\n",
    "        \n",
    "        # build comma separated string with the ids at indexes [start, start+retrieve_per_query)\n",
    "        query_ids = ','.join( [ str(id) for id in Ids[ start : start+retrieve_per_query ] ] )\n",
    "       # print(query_ids)\n",
    "        \n",
    "        s = Entrez.read( Entrez.esummary( db=\"pubmed\", id=query_ids ) )\n",
    "        \n",
    "        # out of the retrieved data, we will keep only a tuple (title, authors, year, DOI), associated with the paper's id.\n",
    "        # (all values converted to native Python formats)\n",
    "        for p in s:\n",
    "            \n",
    "            try:\n",
    "                f = [\n",
    "                    ( int( p['Id'] ), (\n",
    "                        str( p['Title'] ),\n",
    "                        [ str(a) for a in p['AuthorList'] ],\n",
    "                        int( p['PubDate'][:4] ),                # keeps just the publication year\n",
    "                        str( p.get('DOI', '') )            # papers for which no DOI is available get an empty string in their place\n",
    "                        ) )\n",
    "                    ]\n",
    "                #print(int( p['Id'] ))\n",
    "                Summaries.extend( f )\n",
    "            except ValueError as e:\n",
    "                print(\"\\nError with ID \" + p['Id'] + \": \" + str(e))\n",
    "                print(\"Manually remove this ID above and re-run code.\")\n",
    "    \n",
    "    # Save Summaries, as a dictionary indexed by Ids\n",
    "    Summaries = dict( Summaries )\n",
    "    \n",
    "    pickle.dump( Summaries, bz2.BZ2File( Summaries_file, 'wb' ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the first 3 retrieved summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481216\n",
      "('Recovery of surface pose from texture orientation statistics under perspective projection.', ['Warren PA', 'Mamassian P'], 2010, '10.1007/s00422-010-0389-3')\n",
      "481216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{349: ('Acute renal failure.', ['Freedman P', 'Smith EC'], 1975, ''),\n",
       " 480: ('Recognition and significance of maternogenic fetal acidosis during intensive monitoring of labor.',\n",
       "  ['Roversi GD', 'Canussio V', 'Spennacchio M'],\n",
       "  1975,\n",
       "  '10.1515/jpme.1975.3.1.53')}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Ids))\n",
    "\n",
    "print(Summaries[Ids[200001]])\n",
    "print(len(Summaries))\n",
    "{ id : Summaries[id] for id in Ids[1:3] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EFetch: Downloading full records from Entrez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Entrez.efetch()` is the function that will allow us to obtain paper abstracts. Let us start by taking a look at the kind of data it returns when we query PubMed's database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q = Entrez.read( Entrez.efetch(db=\"pubmed\", id='349', retmode=\"xml\") )['PubmedArticle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`q` is a list, with each member corresponding to a queried id. Because here we only queried for one id, its results are then in `q[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 1)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(q), len(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\n",
    "At `q[0]` we find a dictionary containing two keys, the contents of which we print below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bio.Entrez.Parser.DictionaryElement,\n",
       " dict_keys(['MedlineCitation', 'PubmedData']))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(q[0]), q[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReferenceList\n",
      "\t []\n",
      "History\n",
      "\t [DictElement({'Year': '1975', 'Month': '11', 'Day': '1'}, attributes={'PubStatus': 'pubmed'}), DictElement({'Year': '1975', 'Month': '11', 'Day': '1', 'Hour': '0', 'Minute': '1'}, attributes={'PubStatus': 'medline'}), DictElement({'Year': '1975', 'Month': '11', 'Day': '1', 'Hour': '0', 'Minute': '0'}, attributes={'PubStatus': 'entrez'})]\n",
      "PublicationStatus\n",
      "\t ppublish\n",
      "ArticleIdList\n",
      "\t [StringElement('349', attributes={'IdType': 'pubmed'})]\n"
     ]
    }
   ],
   "source": [
    "print_dict( q[0][ 'PubmedData' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key `'MedlineCitation'` maps into another dictionary. In that dictionary, most of the information is contained under the key `'Article'`. To minimize the clutter, below we show the contents of `'MedlineCitation'` excluding its `'Article'` member, and below that we then show the contents of `'Article'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaceFlightMission\n",
      "\t []\n",
      "OtherID\n",
      "\t []\n",
      "OtherAbstract\n",
      "\t []\n",
      "GeneralNote\n",
      "\t []\n",
      "CitationSubset\n",
      "\t ['IM']\n",
      "KeywordList\n",
      "\t []\n",
      "InvestigatorList\n",
      "\t []\n",
      "PMID\n",
      "\t 349\n",
      "DateCompleted\n",
      "\t {'Year': '1976', 'Month': '02', 'Day': '13'}\n",
      "DateRevised\n",
      "\t {'Year': '2010', 'Month': '11', 'Day': '18'}\n",
      "MedlineJournalInfo\n",
      "\t {'Country': 'United States', 'MedlineTA': 'Heart Lung', 'NlmUniqueID': '0330057', 'ISSNLinking': '0147-9563'}\n",
      "MeshHeadingList\n",
      "\t [{'QualifierName': [StringElement('etiology', attributes={'UI': 'Q000209', 'MajorTopicYN': 'N'}), StringElement('metabolism', attributes={'UI': 'Q000378', 'MajorTopicYN': 'N'}), StringElement('therapy', attributes={'UI': 'Q000628', 'MajorTopicYN': 'N'})], 'DescriptorName': StringElement('Acute Kidney Injury', attributes={'UI': 'D058186', 'MajorTopicYN': 'Y'})}, {'QualifierName': [], 'DescriptorName': StringElement('Adolescent', attributes={'UI': 'D000293', 'MajorTopicYN': 'N'})}, {'QualifierName': [], 'DescriptorName': StringElement('Adult', attributes={'UI': 'D000328', 'MajorTopicYN': 'N'})}, {'QualifierName': [], 'DescriptorName': StringElement('Aged', attributes={'UI': 'D000368', 'MajorTopicYN': 'N'})}, {'QualifierName': [], 'DescriptorName': StringElement('Child', attributes={'UI': 'D002648', 'MajorTopicYN': 'N'})}, {'QualifierName': [], 'DescriptorName': StringElement('Humans', attributes={'UI': 'D006801', 'MajorTopicYN': 'N'})}, {'QualifierName': [], 'DescriptorName': StringElement('Hydrogen-Ion Concentration', attributes={'UI': 'D006863', 'MajorTopicYN': 'N'})}, {'QualifierName': [StringElement('physiopathology', attributes={'UI': 'Q000503', 'MajorTopicYN': 'N'})], 'DescriptorName': StringElement('Kidney', attributes={'UI': 'D007668', 'MajorTopicYN': 'N'})}, {'QualifierName': [], 'DescriptorName': StringElement('Middle Aged', attributes={'UI': 'D008875', 'MajorTopicYN': 'N'})}]\n"
     ]
    }
   ],
   "source": [
    "print_dict( { k:v for k,v in q[0][ 'MedlineCitation' ].items() if k!='Article' } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELocationID\n",
      "\t []\n",
      "Language\n",
      "\t ['eng']\n",
      "ArticleDate\n",
      "\t []\n",
      "Journal\n",
      "\t {'ISSN': StringElement('0147-9563', attributes={'IssnType': 'Print'}), 'JournalIssue': DictElement({'Volume': '4', 'Issue': '6', 'PubDate': {'Year': '1975', 'Season': 'Nov-Dec'}}, attributes={'CitedMedium': 'Print'}), 'Title': 'Heart & lung : the journal of critical care', 'ISOAbbreviation': 'Heart Lung'}\n",
      "ArticleTitle\n",
      "\t Acute renal failure.\n",
      "Pagination\n",
      "\t {'StartPage': '873', 'EndPage': '878', 'MedlinePgn': '873-8'}\n",
      "Abstract\n",
      "\t {'AbstractText': ['Acute renal failure is a life-threatening situation for which satisfactory treatment is currently available. Death is usually related to concomitant infection rather than uremia. Prompt recognition and treatment of underlying factors that predispose to renal insufficiency can frequently prevent the development of intrinsic acute renal failure.']}\n",
      "AuthorList\n",
      "\t ListElement([DictElement({'AffiliationInfo': [], 'Identifier': [], 'LastName': 'Freedman', 'ForeName': 'P', 'Initials': 'P'}, attributes={'ValidYN': 'Y'}), DictElement({'AffiliationInfo': [], 'Identifier': [], 'LastName': 'Smith', 'ForeName': 'E C', 'Initials': 'EC'}, attributes={'ValidYN': 'Y'})], attributes={'CompleteYN': 'Y'})\n",
      "PublicationTypeList\n",
      "\t [StringElement('Journal Article', attributes={'UI': 'D016428'})]\n"
     ]
    }
   ],
   "source": [
    "print_dict( q[0][ 'MedlineCitation' ][ 'Article' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A paper's abstract can therefore be accessed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{349: 'Acute renal failure is a life-threatening situation for which satisfactory treatment is currently available. Death is usually related to concomitant infection rather than uremia. Prompt recognition and treatment of underlying factors that predispose to renal insufficiency can frequently prevent the development of intrinsic acute renal failure.'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ int(q[0]['MedlineCitation']['PMID']) : str(q[0]['MedlineCitation']['Article']['Abstract']['AbstractText'][0]) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the ids in our dataset refer to books from the [NCBI Bookshelf](http://www.ncbi.nlm.nih.gov/books/), a collection of freely available, downloadable, on-line versions of selected biomedical books. For such ids, `Entrez.efetch()` returns a slightly different structure, where the keys `[u'BookDocument', u'PubmedBookData']` take the place of the `[u'MedlineCitation', u'PubmedData']` keys we saw above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstracts dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now assemble a dataset mapping paper ids to their abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Abstracts_file = 'data/' + search_term + '_Abstracts.pkl.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Abstracts of results: \n",
      "\n",
      "0...................\n",
      "10000...................\n",
      "20000...................\n",
      "30000...................\n",
      "40000...................\n",
      "50000...................\n",
      "60000...................\n",
      "70000...................\n",
      "80000...................\n",
      "90000...................\n",
      "100000...................\n",
      "110000...................\n",
      "120000...................\n",
      "130000...................\n",
      "140000...................\n",
      "150000...................\n",
      "160000...................\n",
      "170000...................\n",
      "180000...................\n",
      "190000...................\n",
      "200000...................\n",
      "210000...................\n",
      "220000...................\n",
      "230000...................\n",
      "240000...................\n",
      "250000...................\n",
      "260000...................\n",
      "270000...................\n",
      "280000...................\n",
      "290000...................\n",
      "300000...................\n",
      "310000...................\n",
      "320000...................\n",
      "330000...................\n",
      "340000...................\n",
      "350000...................\n",
      "360000...................\n",
      "370000...................\n",
      "380000...................\n",
      "390000...................\n",
      "400000...................\n",
      "410000...................\n",
      "420000...................\n",
      "430000...................\n",
      "440000...................\n",
      "450000...................\n",
      "460000...................\n",
      "470000...................\n",
      "480000.."
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "from collections import deque\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "\n",
    "def ch(node, childtype):\n",
    "    return node.getElementsByTagName(childtype)[0]\n",
    "\n",
    "if os.path.exists( Abstracts_file ):\n",
    "    Abstracts = pickle.load( bz2.BZ2File( Abstracts_file, 'rb' ) )\n",
    "else:\n",
    "    # `Abstracts` will be incrementally assembled, by performing multiple queries,\n",
    "    # each returning at most `retrieve_per_query` entries.\n",
    "    Abstracts = deque()\n",
    "    retrieve_per_query = 500\n",
    "    \n",
    "    print('Fetching Abstracts of results: ')\n",
    "    for start in range( 0, len(Ids), retrieve_per_query ):\n",
    "        if (start % 10000 == 0):\n",
    "            print('')\n",
    "            print(start, end='')\n",
    "        else:\n",
    "            print('.', end='')\n",
    "        \n",
    "        # build comma separated string with the ids at indexes [start, start+retrieve_per_query)\n",
    "        query_ids = ','.join( [ str(id) for id in Ids[ start : start+retrieve_per_query ] ] )\n",
    "        \n",
    "        # issue requests to the server, until we get the full amount of data we expect\n",
    "        while True:\n",
    "            try:\n",
    "                #s = Entrez.read( Entrez.efetch(db=\"pubmed\", id=query_ids, retmode=\"xml\" ) )['PubmedArticle']\n",
    "                s = minidom.parse( Entrez.efetch(db=\"pubmed\", id=query_ids, retmode=\"xml\" ) ).getElementsByTagName(\"PubmedArticle\")\n",
    "            except http.client.IncompleteRead:\n",
    "                print('r', end='')\n",
    "                continue\n",
    "            break\n",
    "        \n",
    "        i = 0\n",
    "        for p in s:\n",
    "            abstr = ''\n",
    "            if (p.getElementsByTagName('MedlineCitation')):\n",
    "                citNode = ch(p,'MedlineCitation')\n",
    "                pmid = ch(citNode,'PMID').firstChild.data\n",
    "                articleNode = ch(citNode,'Article')\n",
    "                if (articleNode.getElementsByTagName('Abstract')):\n",
    "                    try:\n",
    "                        abstr = ch(ch(articleNode,'Abstract'),'AbstractText').firstChild.data\n",
    "                    except AttributeError:\n",
    "                        abstr = ch(ch(articleNode,'Abstract'),'AbstractText').toprettyxml(\"  \")\n",
    "                        abstr = re.sub(r\"\\s+\", \" \", re.sub(\"<[^>]*>\", \"\", abstr))\n",
    "            elif (p.getElementsByTagName('BookDocument')):\n",
    "                bookNode = ch(p,'BookDocument')\n",
    "                pmid = ch(bookNode,'PMID').firstChild.data\n",
    "                if (bookNode.getElementsByTagName('Abstract')):\n",
    "                    try:\n",
    "                        abstr = ch(ch(bookNode,'Abstract'),'AbstractText').firstChild.data\n",
    "                    except AttributeError:\n",
    "                        abstr = ch(ch(bookNode,'Abstract'),'AbstractText').toprettyxml(\"  \")\n",
    "                        abstr = re.sub(r\"\\s+\", \" \", re.sub(\"<[^>]*>\", \"\", abstr))\n",
    "            else:\n",
    "                raise Exception('Unrecognized record type, for id %d (keys: %s)' % (Ids[start+i], str(p.keys())) )\n",
    "            \n",
    "            Abstracts.append( (int(pmid), str(abstr)) )\n",
    "            i += 1\n",
    "    \n",
    "    # Save Abstracts, as a dictionary indexed by Ids\n",
    "    Abstracts = dict( Abstracts )\n",
    "    \n",
    "    pickle.dump( Abstracts, bz2.BZ2File( Abstracts_file, 'wb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at one paper's abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acute renal failure is a life-threatening situation for which satisfactory treatment is currently available. Death is usually related to concomitant infection rather than uremia. Prompt recognition and treatment of underlying factors that predispose to renal insufficiency can frequently prevent the development of intrinsic acute renal failure.\n",
      "480451\n",
      "79763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Abstracts[349])\n",
    "print(len(Abstracts))\n",
    "print(Ids[349])\n",
    "dict_items_list = list(Abstracts.items())\n",
    "print('------',dict_items_list[9990:101002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELink: Searching for related items in NCBI Entrez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how to obtain paper citations with Entrez, we will first assemble a small set of PubMed IDs, and then query for their citations.\n",
    "To that end, we search here for papers published in the Nature journal with our given keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['39415022', '39385031', '39385025', '39358508', '39232164', '39143217', '39143211', '39111359', '39085614', '39048818', '39020174', '39020165', '38961301', '38961291', '38961289', '38840020', '38839966', '38776963', '38750366', '38750358']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CA_search_term = search_term+'[TIAB] AND Nature[JOUR]'\n",
    "CA_ids = Entrez.read( Entrez.esearch( db=\"pubmed\", term=CA_search_term ) )['IdList']\n",
    "CA_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'39415022': ('A bacterial immunity protein directly senses two disparate phage proteins.',\n",
       "  ['Zhang T', 'Cepauskas A', 'Nadieina A', 'Thureau A', \"Coppieters 't Wallant K\", 'Martens C', 'Lim DC', 'Garcia-Pino A', 'Laub MT'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-08039-y'),\n",
       " '39385031': ('A modular circuit coordinates the diversification of courtship strategies.',\n",
       "  ['Coleman RT', 'Morantte I', 'Koreman GT', 'Cheng ML', 'Ding Y', 'Ruta V'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-08028-1'),\n",
       " '39385025': ('Structural basis of mRNA decay by the human exosome-ribosome supercomplex.',\n",
       "  ['Kögel A', 'Keidel A', 'Loukeri MJ', 'Kuhn CC', 'Langer LM', 'Schäfer IB', 'Conti E'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-08015-6'),\n",
       " '39358508': ('Rapid homologue juxtaposition during meiotic chromosome pairing.',\n",
       "  ['Nozaki T', 'Weiner B', 'Kleckner N'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07999-5'),\n",
       " '39232164': ('A pathology foundation model for cancer diagnosis and prognosis prediction.',\n",
       "  ['Wang X', 'Zhao J', 'Marostica E', 'Yuan W', 'Jin J', 'Zhang J', 'Li R', 'Tang H', 'Wang K', 'Li Y', 'Wang F', 'Peng Y', 'Zhu J', 'Zhang J', 'Jackson CR', 'Zhang J', 'Dillon D', 'Lin NU', 'Sholl L', 'Denize T', 'Meredith D', 'Ligon KL', 'Signoretti S', 'Ogino S', 'Golden JA', 'Nasrallah MP', 'Han X', 'Yang S', 'Yu KH'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07894-z'),\n",
       " '39143217': ('Recognition and control of neutrophil extracellular trap formation by MICL.',\n",
       "  ['Malamud M', 'Whitehead L', 'McIntosh A', 'Colella F', 'Roelofs AJ', 'Kusakabe T', 'Dambuza IM', 'Phillips-Brookes A', 'Salazar F', 'Perez F', 'Shoesmith R', 'Zakrzewski P', 'Sey EA', 'Rodrigues C', 'Morvay PL', 'Redelinghuys P', 'Bedekovic T', 'Fernandes MJG', 'Almizraq R', 'Branch DR', 'Amulic B', 'Harvey J', 'Stewart D', 'Yuecel R', 'Reid DM', 'McConnachie A', 'Pickering MC', 'Botto M', 'Iliev ID', 'McInnes IB', 'De Bari C', 'Willment JA', 'Brown GD'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07820-3'),\n",
       " '39143211': ('Substrate binding and inhibition mechanism of norepinephrine transporter.',\n",
       "  ['Ji W', 'Miao A', 'Liang K', 'Liu J', 'Qi Y', 'Zhou Y', 'Duan X', 'Sun J', 'Lai L', 'Wu JX'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07810-5'),\n",
       " '39111359': ('A virally encoded tRNA neutralizes the PARIS antiviral defence system.',\n",
       "  ['Burman N', 'Belukhina S', 'Depardieu F', 'Wilkinson RA', 'Skutel M', 'Santiago-Frangos A', 'Graham AB', 'Livenskyi A', 'Chechenina A', 'Morozova N', 'Zahl T', 'Henriques WS', 'Buyukyoruk M', 'Rouillon C', 'Saudemont B', 'Shyrokova L', 'Kurata T', 'Hauryliuk V', 'Severinov K', 'Groseille J', 'Thierry A', 'Koszul R', 'Tesson F', 'Bernheim A', 'Bikard D', 'Wiedenheft B', 'Isaev A'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07874-3'),\n",
       " '39085614': ('FANCD2-FANCI surveys DNA and recognizes double- to single-stranded junctions.',\n",
       "  ['Alcón P', 'Kaczmarczyk AP', 'Ray KK', 'Liolios T', 'Guilbaud G', 'Sijacki T', 'Shen Y', 'McLaughlin SH', 'Sale JE', 'Knipscheer P', 'Rueda DS', 'Passmore LA'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07770-w'),\n",
       " '39048818': ('Molecular basis of human noradrenaline transporter reuptake and inhibition.',\n",
       "  ['Tan J', 'Xiao Y', 'Kong F', 'Zhang X', 'Xu H', 'Zhu A', 'Liu Y', 'Lei J', 'Tian B', 'Yuan Y', 'Yan C'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07719-z'),\n",
       " '39020174': ('Tubulin code eraser CCP5 binds branch glutamates by substrate deformation.',\n",
       "  ['Chen J', 'Zehr EA', 'Gruschus JM', 'Szyk A', 'Liu Y', 'Tanner ME', 'Tjandra N', 'Roll-Mecak A'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07699-0'),\n",
       " '39020165': ('Bacteria conjugate ubiquitin-like proteins to interfere with phage assembly.',\n",
       "  ['Hör J', 'Wolf SG', 'Sorek R'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07616-5'),\n",
       " '38961301': ('Targeting pericentric non-consecutive motifs for heterochromatin initiation.',\n",
       "  ['Ma R', 'Zhang Y', 'Zhang J', 'Zhang P', 'Liu Z', 'Fan Y', 'Wang HT', 'Zhang Z', 'Zhu B'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07640-5'),\n",
       " '38961291': ('Molecular definition of the endogenous Toll-like receptor signalling pathways.',\n",
       "  ['Fisch D', 'Zhang T', 'Sun H', 'Ma W', 'Tan Y', 'Gygi SP', 'Higgins DE', 'Kagan JC'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07614-7'),\n",
       " '38961289': ('Adenosine signalling to astrocytes coordinates brain metabolism and function.',\n",
       "  ['Theparambil SM', 'Kopach O', 'Braga A', 'Nizari S', 'Hosford PS', 'Sagi-Kiss V', 'Hadjihambi A', 'Konstantinou C', 'Esteras N', 'Gutierrez Del Arroyo A', 'Ackland GL', 'Teschemacher AG', 'Dale N', 'Eckle T', 'Andrikopoulos P', 'Rusakov DA', 'Kasparov S', 'Gourine AV'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07611-w'),\n",
       " '38840020': ('China seeks global impact and recognition.',\n",
       "  ['Baker S'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/d41586-024-01595-3'),\n",
       " '38839966': ('Observation of edge states derived from topological helix chains.',\n",
       "  ['Nakayama K', 'Tokuyama A', 'Yamauchi K', 'Moriya A', 'Kato T', 'Sugawara K', 'Souma S', 'Kitamura M', 'Horiba K', 'Kumigashira H', 'Oguchi T', 'Takahashi T', 'Segawa K', 'Sato T'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07484-z'),\n",
       " '38776963': ('Bitter taste TAS2R14 activation by intracellular tastants and cholesterol.',\n",
       "  ['Hu X', 'Ao W', 'Gao M', 'Wu L', 'Pei Y', 'Liu S', 'Wu Y', 'Zhao F', 'Sun Q', 'Liu J', 'Jiang L', 'Wang X', 'Li Y', 'Tan Q', 'Cheng J', 'Yang F', 'Yang C', 'Sun J', 'Hua T', 'Liu ZJ'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07569-9'),\n",
       " '38750366': ('Physiological temperature drives TRPM4 ligand recognition and gating.',\n",
       "  ['Hu J', 'Park SJ', 'Walter T', 'Orozco IJ', \"O'Dea G\", 'Ye X', 'Du J', 'Lü W'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07436-7'),\n",
       " '38750358': ('Dimerization and antidepressant recognition at noradrenaline transporter.',\n",
       "  ['Zhang H', 'Yin YL', 'Dai A', 'Zhang T', 'Zhang C', 'Wu C', 'Hu W', 'He X', 'Pan B', 'Jin S', 'Yuan Q', 'Wang MW', 'Yang D', 'Xu HE', 'Jiang Y'],\n",
       "  '2024',\n",
       "  'Nature',\n",
       "  '10.1038/s41586-024-07437-6')}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CA_summ = {\n",
    "    p['Id'] : ( p['Title'], p['AuthorList'], p['PubDate'][:4], p['FullJournalName'], p.get('DOI', '') )\n",
    "    for p in Entrez.read( Entrez.esummary(db=\"pubmed\", id=','.join( CA_ids )) )\n",
    "    }\n",
    "CA_summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we restricted our search to papers in an open-access journal, you can then follow their DOIs to freely access their PDFs at the journal's website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now issue calls to `Entrez.elink()` using these PubMed IDs, to retrieve the IDs of papers that cite them.\n",
    "The database from which the IDs will be retrieved is [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/), a free digital database of full-text scientific literature in the biomedical and life sciences.\n",
    "\n",
    "A complete list of the kinds of links you can retrieve with `Entrez.elink()` can be found [here](http://eutils.ncbi.nlm.nih.gov/entrez/query/static/entrezlinks.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CA_citing = {\n",
    "    id : Entrez.read( Entrez.elink(\n",
    "            cmd = \"neighbor\",               # ELink command mode: \"neighbor\", returns\n",
    "                                            #     a set of UIDs in `db` linked to the input UIDs in `dbfrom`.\n",
    "            dbfrom = \"pubmed\",              # Database containing the input UIDs: PubMed\n",
    "            db = \"pmc\",                     # Database from which to retrieve UIDs: PubMed Central\n",
    "            LinkName = \"pubmed_pmc_refs\",   # Name of the Entrez link to retrieve: \"pubmed_pmc_refs\", gets\n",
    "                                            #     \"Full-text articles in the PubMed Central Database that cite the current articles\"\n",
    "            from_uid = id                   # input UIDs\n",
    "            ) )\n",
    "    for id in CA_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'349'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mCA_citing\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m349\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: '349'"
     ]
    }
   ],
   "source": [
    "CA_citing['349']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have in `CA_citing[paper_id][0]['LinkSetDb'][0]['Link']` the list of papers citing `paper_id`. To get it as just a list of ids, we can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11476048',\n",
       " '11417361',\n",
       " '11411926',\n",
       " '11389603',\n",
       " '11364771',\n",
       " '11071384',\n",
       " '10557695']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cits = [ l['Id'] for l in CA_citing['39020165'][0]['LinkSetDb'][0]['Link'] ]\n",
    "cits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one more step is needed, as what we have now are PubMed Central IDs, and not PubMed IDs. Their conversion can be achieved through an additional call to `Entrez.elink()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LinkSetDbHistory': [], 'ERROR': [], 'LinkSetDb': [{'Link': [{'Id': '39215040'}, {'Id': '39149900'}, {'Id': '38712175'}, {'Id': '37808811'}], 'DbTo': 'pubmed', 'LinkName': 'pmc_pubmed'}], 'DbFrom': 'pmc', 'IdList': ['11476048', '11417361', '11411926', '11389603', '11364771', '11071384', '10557695']}]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cits_pm = Entrez.read( Entrez.elink( dbfrom=\"pmc\", db=\"pubmed\", LinkName=\"pmc_pubmed\", from_uid=\",\".join(cits)) )\n",
    "cits_pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ZipFile' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ids_map \u001b[38;5;241m=\u001b[39m { pmc_id : link[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m (pmc_id,link) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcits_pm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIdList\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcits_pm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLinkSetDb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLink\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m }\n\u001b[1;32m      2\u001b[0m ids_map\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ZipFile' object is not callable"
     ]
    }
   ],
   "source": [
    "ids_map = { pmc_id : link['Id'] for (pmc_id,link) in zip(cits_pm[0]['IdList'], cits_pm[0]['LinkSetDb'][0]['Link']) }\n",
    "ids_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to check these papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'39215040': ('Tail assembly interference is a common strategy in bacterial antiviral defenses.',\n",
       "  ['He L', 'Miguel-Romero L', 'Patkowski JB', 'Alqurainy N', 'Rocha EPC', 'Costa TRD', 'Fillol-Salom A', 'Penadés JR'],\n",
       "  '2024',\n",
       "  'Nature communications',\n",
       "  '10.1038/s41467-024-51915-4'),\n",
       " '39149900': ('A phage satellite manipulates the viral DNA packaging motor to inhibit phage and promote satellite spread.',\n",
       "  ['Boyd CM', 'Seed KD'],\n",
       "  '2024',\n",
       "  'Nucleic acids research',\n",
       "  '10.1093/nar/gkae675'),\n",
       " '37808811': ('Bacterial antiviral defense pathways encode eukaryotic-like ubiquitination systems.',\n",
       "  ['Chambers LR', 'Ye Q', 'Cai J', 'Gong M', 'Ledvina HE', 'Zhou H', 'Whiteley AT', 'Suhandynata RT', 'Corbett KD'],\n",
       "  '2023',\n",
       "  'bioRxiv : the preprint server for biology',\n",
       "  '10.1101/2023.09.26.559546')}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{   p['Id'] : ( p['Title'], p['AuthorList'], p['PubDate'][:4], p['FullJournalName'], p.get('DOI', '') )\n",
    "    for p in Entrez.read( Entrez.esummary(db=\"pubmed\", id=','.join( ids_map.values() )) )\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now seen all the steps required to assemble a dataset of citations to each of the papers in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Citations_file = 'data/' + search_term + '_Citations.pkl.bz2'\n",
    "Citations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least one server query will be issued per paper in `Ids`. Because NCBI allows for at most 3 queries per second (see [here](http://biopython.org/DIST/docs/api/Bio.Entrez-pysrc.html#_open)), this dataset will take a long time to assemble. Should you need to interrupt it for some reason, or the connection fail at some point, it is safe to just rerun the cell below until all data is collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................\tsaving at checkpoint 69890\n",
      "...........\n",
      "70000........r\n",
      "..............................\tsaving at checkpoint 70372\n",
      "................................................\tsaving at checkpoint 70854\n",
      "..............\n",
      "71000..................................\tsaving at checkpoint 71336\n",
      "................................................\tsaving at checkpoint 71818\n",
      "..................\n",
      "72000..............................\tsaving at checkpoint 72300\n",
      ".................................................\tsaving at checkpoint 72782\n",
      ".....................\n",
      "73000...........................\tsaving at checkpoint 73264\n",
      "................................................\tsaving at checkpoint 73746\n",
      ".........................\n",
      "74000.......................\tsaving at checkpoint 74228\n",
      "................................................\tsaving at checkpoint 74710\n",
      ".............................\n",
      "75000....................\tsaving at checkpoint 75192\n",
      "................................................\tsaving at checkpoint 75674\n",
      "................................\n",
      "76000................\tsaving at checkpoint 76156\n",
      "................................................\tsaving at checkpoint 76638\n",
      "....................................\n",
      "77000............\tsaving at checkpoint 77120\n",
      ".................................................\tsaving at checkpoint 77602\n",
      ".......................................\n",
      "78000.........\tsaving at checkpoint 78084\n",
      "................................................\tsaving at checkpoint 78566\n",
      "...........................................\n",
      "79000.....\tsaving at checkpoint 79048\n",
      "................................................\tsaving at checkpoint 79530\n",
      "...............................................\n",
      "80000..\tsaving at checkpoint 80012\n",
      "......................................incompleteRead, continuing\n",
      "incompleteRead, continuing\n",
      "incompleteRead, continuing\n",
      "incompleteRead, continuing\n",
      "incompleteRead, continuing\n",
      "..........\tsaving at checkpoint 80494\n",
      "................................................\tsaving at checkpoint 80976\n",
      "..\n",
      "81000..............................................\tsaving at checkpoint 81458\n",
      "................................................\tsaving at checkpoint 81940\n",
      "......\n",
      "82000............................................................\tsaving at checkpoint 83386\n",
      "................................................\tsaving at checkpoint 83868\n",
      ".............\n",
      "84000...................................\tsaving at checkpoint 84350\n",
      ".................................................\tsaving at checkpoint 84832\n",
      "."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# query for papers archived in PubMed Central that cite the paper with PubMed ID `pm_id`\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m         c \u001b[38;5;241m=\u001b[39m \u001b[43mEntrez\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mEntrez\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melink\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbfrom\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpubmed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpmc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLinkName\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpubmed_pmc_refs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpm_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         c \u001b[38;5;241m=\u001b[39m c[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinkSetDb\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(c) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;66;03m# no citations found for the current paper\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/Bio/Entrez/__init__.py:503\u001b[0m, in \u001b[0;36mread\u001b[0;34m(handle, validate, escape, ignore_errors)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataHandler\n\u001b[1;32m    502\u001b[0m handler \u001b[38;5;241m=\u001b[39m DataHandler(validate, escape, ignore_errors)\n\u001b[0;32m--> 503\u001b[0m record \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m record\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/Bio/Entrez/Parser.py:392\u001b[0m, in \u001b[0;36mDataHandler.read\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile should be opened in binary mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 392\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParseFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m expat\u001b[38;5;241m.\u001b[39mExpatError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mStartElementHandler:\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;66;03m# We saw the initial <!xml declaration, so we can be sure that\u001b[39;00m\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;66;03m# we are parsing XML data. Most likely, the XML file is\u001b[39;00m\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;66;03m# corrupted.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:493\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked:\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readinto_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:588\u001b[0m, in \u001b[0;36mHTTPResponse._readinto_chunked\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m         chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_chunk_left\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m total_bytes\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:556\u001b[0m, in \u001b[0;36mHTTPResponse._get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# toss the CRLF at the end of the chunk\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 556\u001b[0m     chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_next_chunk_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:516\u001b[0m, in \u001b[0;36mHTTPResponse._read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_next_chunk_size\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# Read the next chunk size from the file\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1270\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1268\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1269\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1128\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "from urllib.error import HTTPError\n",
    "if Citations == [] and os.path.exists( Citations_file ):\n",
    "    Citations = pickle.load( bz2.BZ2File( Citations_file, 'rb' ) )\n",
    "\n",
    "if len(Citations) < len(Ids):\n",
    "    \n",
    "    i = len(Citations)\n",
    "    checkpoint = int(len(Ids) / 1000) + 1      # save to hard drive at every 0.1% of Ids fetched\n",
    "    \n",
    "    for pm_id in Ids[i:]:               # either starts from index 0, or resumes from where we previously left off\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # query for papers archived in PubMed Central that cite the paper with PubMed ID `pm_id`\n",
    "                c = Entrez.read( Entrez.elink( dbfrom = \"pubmed\", db=\"pmc\", LinkName = \"pubmed_pmc_refs\", id=str(pm_id) ) )\n",
    "                \n",
    "                c = c[0]['LinkSetDb']\n",
    "                if len(c) == 0:\n",
    "                    # no citations found for the current paper\n",
    "                    c = []\n",
    "                    #print(\"no links for\",pm_id)\n",
    "                else:\n",
    "                    c = [ l['Id'] for l in c[0]['Link'] ]\n",
    "                    \n",
    "                    # convert citations from PubMed Central IDs to PubMed IDs\n",
    "                    p = []\n",
    "                    retrieve_per_query = 100\n",
    "                    for start in range( 0, len(c), retrieve_per_query ):\n",
    "                        query_ids = ','.join( c[start : start+retrieve_per_query] )\n",
    "                        r = Entrez.read( Entrez.elink( dbfrom=\"pmc\", db=\"pubmed\", LinkName=\"pmc_pubmed\", from_uid=query_ids ) )\n",
    "                        # select the IDs. If no matching PubMed ID was found, [] is returned instead\n",
    "                        p.extend( [] if r[0]['LinkSetDb']==[] else [ int(link['Id']) for link in r[0]['LinkSetDb'][0]['Link'] ] )\n",
    "                    c = p\n",
    "            \n",
    "            except http.client.BadStatusLine:\n",
    "                # Presumably, the server closed the connection before sending a valid response. Retry until we have the data.\n",
    "                print('r')\n",
    "                continue\n",
    "            except http.client.IncompleteRead:\n",
    "                print('incompleteRead, continuing')\n",
    "                continue\n",
    "            except HTTPError:\n",
    "                print('r')\n",
    "                continue\n",
    "            break\n",
    "        \n",
    "        Citations.append( (pm_id, c) )\n",
    "       # print(pm_id,' :',c)\n",
    "        if (i % 1000 == 0):\n",
    "            print('')\n",
    "            print(i, end='')\n",
    "        if (i % 10 == 0):\n",
    "            print('.', end='')\n",
    "        i += 1\n",
    "        \n",
    "        if i % checkpoint == 0:\n",
    "            print('\\tsaving at checkpoint', i)\n",
    "            pickle.dump( Citations, bz2.BZ2File( Citations_file, 'wb' ) )\n",
    "    \n",
    "    print('\\n done.')\n",
    "    \n",
    "    # Save Citations, as a dictionary indexed by Ids\n",
    "    Citations = dict( Citations )\n",
    "    \n",
    "    pickle.dump( Citations, bz2.BZ2File( Citations_file, 'wb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see that we have indeed obtained the data we expected, you can match the ids below, with the ids listed at the end of last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The male determinant of self-incompatibility in Brassica.',\n",
       " ['Schopfer CR', 'Nasrallah ME', 'Nasrallah JB'],\n",
       " 1999,\n",
       " '10.1126/science.286.5445.1697')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Citations))\n",
    "Citations[84842]\n",
    "Summaries[10576728]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do we go from here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above generates multiple local files, containing the datasets we'll be working with. Loading them into memory is a matter of just issuing a call like<br>\n",
    "``data = pickle.load( bz2.BZ2File( data_file, 'rb' ) )``.\n",
    "\n",
    "The Entrez module will therefore no longer be needed, unless you wish to extend your data processing with additional information retrieved from PubMed.\n",
    "\n",
    "Should you be interested in looking at alternative ways to handle the data, have a look at the [sqlite3](http://docs.python.org/3/library/sqlite3.html) module included in Python's standard library, or [Pandas](http://pandas.pydata.org/), the Python Data Analysis Library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
